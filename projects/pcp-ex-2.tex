\documentclass{article}

\input{usepackage.tex}
\input{newcommands.tex}

\title{PCP - Huji Course, Ex 2.} 
\author{David Ponarovsky}
%\abstract{We propose a new simple construction based on Tanner Codes, which yields a good LDPC testable code.} 

\newcommand{\FF}{\mathbb{F}_{q}}
\newcommand{\Chi}[1]{\chi_{ \left\{ #1  \right\} } }
\begin{document}
\maketitle

\section{Ex 1. Sumchecking with coefficients.}

We would like to verify that a given polynomial box $P$ satisfies that $ \sum_{x\in [d]^{m}}{ \varphi\left( x \right) f_{P}\left( x \right) } = 0 $  by accessing to at most $O\left(md\right)$ variables. For any function $\varphi : [d]^{m} \rightarrow \FF$. Denote by $\varphi^{\prime} : \FF^{m} \rightarrow \FF$ the extension of $\varphi$ into a polynomial over $\FF^{m}$. We saw in that lectures (and also in the previews assignment) that there is such a unique extension. 

We would like to verify that a given polynomial box $P$ satisfies $\sum_{x\in [d]^{m}}{ \varphi\left( x \right) f_{P}\left( x \right) } = 0$ by accessing at most $O\left(md\right)$ variables. For any function $\varphi : [d]^{m} \rightarrow \FF$, denote by $\varphi^{\prime} : \FF^{m} \rightarrow \FF$ the unique extension of $\varphi$ into a polynomial over $\FF^{m}$, which we saw in the lectures and the previous assignment.



We are going to split the section into two, first we are going to show how to verify that $\sum_{x \in [d]^{m}}{ f_{P}\left( x \right) } = 0 $. When the polynomial is a function into $\FF$. ( I think, but not sure, that in the lecture we saw only the case when $q = 2$ ). Then in the second part we will show how can one redact the coefficients case into the non-coefficients case. Finally, in the last part, we combine all together to show that the construction achieve the requirements. 

We will divide this section into two parts. First, we will demonstrate how to verify that $\sum_{x \in [d]^{m}}{ f_{P}\left( x \right) } = 0$ when the polynomial is a function into $\FF$. (I believe the lecture only covered the case when $q = 2$). In the second part, we will explain how to convert the coefficients case to the non-coefficients case. Finally, we will combine all of this to show that the construction meets the requirements.

%\paragraph{Comment.} The following soultion acsses to $\Theta\left( m^{2} \cdot d \right)$ variabels, that in contrast to the requirement that verfication will acsses to at most $\Theta\left( m\cdot d \right)$ variabels. 

\subsection{Over non binary field. } Let's define a series of polynomial boxes $f_{i}$ such that: 

\begin{equation*}
  \begin{split}
    f_{0} & = f \\ 
  f_{i+1} \left( x_{1}, .. , x_{m-i} \right) & = \sum_{y \in [d] } f_{i}\left(x_{1}, .. , x_{m-i}, x_{m-i+1} = y \right)
  \end{split}
\end{equation*}
Our verifier will ask for a proof which is a list of $f_{0}, f_{1}, f_{2} .., f_{m}$. Now, notice that if $f$ is an honest assignment then $f_{m}$ is just the summation of $f$ over the cube $[d]^{m}$. So it sufficient to show the existences of verifier that reject with heigh probability any string far from been encoded by the previews structure.

Our verifier will ask for a proof, which is a list of $f_{0}, f_{1}, f_{2}, \dots, f_{m}$. Now, if $f$ is an honest assignment, then $f_{m}$ is simply the sum of $f$ over the cube $[d]^{m}$. Thus, it is sufficient to show the existence of a verifier that rejects with high probability any string that is far from being encoded by the previous structure.

\begin{algorithm}[H]
  Sample a line and a point and use them to test any of the polynomials $f_{i}$ by the line versus point test consuming $\Theta\left( m\cdot d \right)$ of randomness.  (Not necessary if we can assume the validity of each of the polynomials boxes) \\  
 %Check for any $i \sim [m]$ that $f_{i}$ is a codeword of the polynomial code in $m$ variables at degree at most $m\cdot d$. Use the standart interpolation test.        
    $r_{1}, r_{2} .., r_{m} \leftarrow$ sample uniformly  $m$ points of $[d]$ \\
    \For{$ i \in [1,m]$} {
      Check if $f_{i+1}\left( r_{1} , .., r_{m-i-1}, x_{m-i} \right) - \sum_{y \in [d]}{f_{i} \left( r_{1}, r_{2}, .., r_{m-i-1},x_{m-i}, y \right)} $ is the zero polynomial by a random test that uses at most single query. (Here $x_{m-i} \in [d]$ is the only variable)  \\
      \ \\
      If not then reject.  
    }
    Accept if $f_{m} = 0 $  
\end{algorithm}

\begin{proof} For convenient let's denote by $g_{i}\left( x_{m-i} \right)$ the difference that been queried in line number $4$. 
  \begin{enumerate}
    \item Completeness. Easy. If the assignment is honest then by definition $g_{i} = 0$ for any $i \in [m]$ and therefore for any $x_{m-i}$ we will have that $g_{i} \left( x_{m-i} \right) = 0$. So, in that case iteration will pass. And whole proof will be aspected  with probability $1$.  
    \item Soundness. Assume an adversary input in which $f_{m} = 0$ but $\sum_{\mathbf{x} \in [d]^{m}}{f_{0}(x)}\neq 0$. (The case which $f_{m} \neq 0$ is not interesting). Now, observers that this can happens only if either at least one the $f_{i}$ isn't setted according the definition above or at least of the $f_{i}$ is not pass the $m\cdot d$-degree polynomial test with probability grater than $1 - \frac{m\cdot d}{q}$. 

      In the first case, there exists at least one $i$ such that $g_{i}\neq 0$:
%      \begin{equation}
%        \label{equ:sat}
%        \begin{split}
%          & f_{i+1} \left( x_{1}, .. , x_{m-i} \right)  \neq  \sum_{y \in [d] } f_{i}\left(x_{1}, .. , x_{m-i}, x_{m-i+1} = y \right) \\
%          \Rightarrow & f_{i+1} \left( x_{1}, .. , x_{m-i} \right) -  \sum_{y \in [d] } f_{i}\left(x_{1}, .. , x_{m-i}, x_{m-i+1} = y \right) \neq 0  
%        \end{split}
%      \end{equation}
      Now the probability to reject the proof is greater than the probability to catch a nonzero point when probing $g_{i}$. As we are assuming that all the $f_{i}$ pass with probability grater than $1 - \frac{m\cdot d}{q}$ the polynomials test (the second case, in which they aren't, is treated next) it follows that there exists a polynomial $\tilde{f}_{i}$ at degree at most $md$ that close to $f_{i}$ in the sense that we can assume that with probability $1 - \Theta(\frac{md}{q})$ we query only points from $\tilde{f}_{i}$. 

      The same holds for $f_{i+1}$ and therefore we can say that with probability at least $1 - \Theta(\frac{md}{q})$ difference $g_{i}$ also agrees with a polynomial at degree at most $m\cdot d$, denote it by $\tilde{g}_{i}$. Thus the probability to fall on non zero point is greater than the probability to fall on point which both $g_{i},\tilde{g}_{i}$ agree on and it's zero. Combining it all together we get that with probability at most 
      \begin{equation*}
        \begin{split}
          \prb{\text{accept}} \le \  & \prb{\text{checks fall only on zeros of } g_{i}} \\
          & \ \ = 1 - \prb{ g_{i}(x) \neq 0 } \\ 
          & \ \ \le  1 - \prb{ \left\{ \text{ querying points from } \tilde{f}_{i}, \tilde{f}_{i+1}  \right\} \cap \left\{  \tilde{g}_i(x) \neq 0 \right\} } \\
          & \ \ =  1 - \left( 1 - \Theta\left(\frac{m\cdot d}{q} \right) \right)^{O(1)} \\ 
          & \ \ = \Theta\left(\frac{m\cdot d}{q} \right)
        \end{split}
      \end{equation*}
    the test accept.


      We use similar arguments to treat the case in which one of the functions $f_{i}$ is not close to a polynomial. As it given in the question that the validity of $f_{P}$ can be assumed, we guess that intent was not to dig down into a multi variable polynomials verification. Thus we just mention without a proof that the probability for rejecting is grater than the probability that $f_{i}$ fail in a low degree polynomial test and that also happens with probability at least $1 - \frac{m\cdot d}{q}$. 
  \end{enumerate}
\end{proof}

\subsection{ Coefficients $\mapsto$ non-coefficients. }
By the fact that for any pair of polynomials $f,g$ the degree of their product is at most the sum of their degrees  $\deg f \cdot g \le \deg f + \deg g$  we can redact the problem of verifying whether the weight summation is zero by considering the summation of the polynomial $\varphi^{\prime} \cdot f$ over the cube $[d]^{m}$. When $\varphi^{\prime}$ is the extension of $\varphi$  to $\FF^{m} \rightarrow \FF$. 

Let's denote by $\xi = f\cdot\varphi$ and the corresponding polynomial box by $\xi_{P} = f_{P} \cdot \varphi_{P}$. Note that by the uniqueness of the extension of both $\varphi$ and $f$ into $\FF^{m}\rightarrow \FF$ we get also that the extension of $\xi$ is unique and $\xi_{P}$ is well defined.  

Our verifier will take as proof: 

\begin{enumerate}
  \item The polynomials $f,\varphi, \xi$ 
  \item Their corresponding polynomial boxes $f_{P},\varphi_{P}, \xi_{P}$ 
  \item The polynomials boxes correspond to $\xi_{0}, \xi_{1} .. \xi_{m}$ as defined in the previews section.  
\end{enumerate}

\begin{algorithm}[H]
  Sample uniformly random $x \sim  [d]^{m}$ and check that $\varphi^{\prime}\left( x \right) = \varphi\left( x \right)$   \\
  Check that $\varphi$ is a polynomial at degree at most $d\cdot m$. \\
  Check that the degree of $\xi$ is at most $2md$ by querying the $\xi_{P}$.\\ 
  Check that the polynomial $f\cdot \varphi^{\prime} - \xi$ is the zero polynomial by querying the boxes $f_{P},\varphi_{P},\xi_{P}$. \\
  Using the sumcheck verifier on $\xi, \xi_{1},\xi_{2} .. \xi_{m}$, accept if the summation of $\xi$ over the cube $[d]^{m}$ is zero.  
\end{algorithm}

\begin{proof} \ 


  \begin{enumerate}
    \item Completeness. The key point here is the fact that the extension $\xi_{P} = \varphi_{P} \cdot f_{P}$ is unique and agrees with $\varphi\cdot f$ on all points in the cube $[d]^{m}$. If the proof is honest then all validity of the input check at lines $1-4$ are pass with probability $1$. Then we get by the completeness of the sumcheck verification that if indeed $\sum_{x \in [d]^{m}}{ \xi(x) }$ $ = \sum_{x \in [d]^{m}}{ \varphi(x)f_{P}(x) } = 0$ then line number $5$ also passes with probability $1$. 
    \item Soundness. Returns exactly on the soundness proof in the above section, when here we apply also the idea that if the inputs passes the validity tests in lines $1-4$ with probability grater than $1-\Theta(\frac{m\cdot d}{1})$ then there is a valid input which is close enough to the given input and by conditioning on querying only points that both of them agree on.  
  \end{enumerate}
\end{proof}

\section{Ex 2.}
The question concerns with the following test:

\begin{algorithm}[H]
  Choose $x,y \in \{\pm1\}^{k}$ independently. \\
  Choose $\mu \in \{\pm\}$. \\ 
  Choose a random noise $z \in \{\pm\}^{k}$ such that $z_{i}$ gets $+1$ with probability $1-\varepsilon$.  \\
  Accept if $\mu f\left( \mu x \right) \cdot g\left( y \right) = f\left(z \cdot x c^{-1}\left( y \right)  \right)$

\end{algorithm}


\subsection{2.a.} Let $f = \Chi{i}, g = \Chi{j}$ and $j = c(i)$. In that case it holds that: 

\begin{equation*}
  \begin{split}
    \mu f\left( \mu x \right) \cdot g\left( y \right)  &= \mu \Chi{i}\left( \mu x \right)\Chi{j}\left( y \right) = \mu^{2}x_{i}y_{j} = x_{i}y_{j} \\ 
    f\left(z \cdot x c^{-1}\left( y \right)  \right) & = \Chi{i}\left( zx c^{-1}(y) \right) = z_{i}x_{i}y_{j} 
  \end{split}
\end{equation*}
Thus, the test pass only if $z_{i} = 1$ and it given that this event happens with probability $1-\varepsilon$. 

\subsection{2.b.} 
Denote by $\alpha_{I} \in \mathbb{R}$ and $\beta_{I} \in \mathbb{R}$ the coefficients of $f,g$ over the character $\Chi{I}$.  
\begin{equation*}
  \begin{split}
    & \expp{\mu f\left( \mu x \right) \cdot g\left( y \right)f\left(z \cdot x c^{-1}\left( y \right)  \right)} \\
    & = \sum_{I,J,K}\alpha_{I}\alpha_{K}\beta_{J}\expp{ \mu \Chi{I}\left( \mu x \right)\Chi{J}\left( y \right) \Chi{K}\left( zx c^{-1}(y) \right) } \\
    & = \sum_{I,J,K}\alpha_{I}\alpha_{K}\beta_{J}\expp{\expp{ \mu \Chi{I}\left( \mu x \right)\Chi{J}\left( y \right) \Chi{K}\left( zx c^{-1}(y) \right)|\mu}} \\ 
    & = \sum_{I,J,K}\alpha_{I}\alpha_{K}\beta_{J}\frac{1}{2}\left( \left( - 1 \right)^{|I|+1}  + 1\right) \expp{  \Chi{I}\left(  x \right)\Chi{J}\left( y \right) \Chi{K}\left( zx c^{-1}(y) \right) }  
  \end{split}
\end{equation*}

Thus, all the elements in which $|I|$ is even contribute zero for the exception. Now, let's apply the conditional expectation formula again conditioning over $I,J,K, x,y$:   
\begin{equation*}
  \begin{split}
    &  = \sum_{I,J,K, |I| \text{is odd} }\alpha_{I}\alpha_{K}\beta_{J} \expp{\expp{ \Chi{I}\left(  x \right)\Chi{J}\left( y \right) \Chi{K}\left( zx c^{-1}(y) \right) | I,J,K }} \\  
    &  = \sum_{I,J,K, |I| \text{is odd} }\alpha_{I}\alpha_{K}\beta_{J} \expp{ \sum_{ \xi = 0}^{|K|}{ { |K| \choose \xi } \left( -\varepsilon \right)^{\xi} \left( 1 - \varepsilon \right)^{|K|- \xi} \Chi{I}\left(  x \right)\Chi{J}\left( y \right) \Chi{K}\left( x c^{-1}(y) \right) }} \\  
    &  = \sum_{I,J,K, |I| \text{is odd} }\alpha_{I}\alpha_{K}\beta_{J} \expp{ \left( 1 -2 \varepsilon \right)^{|K|} \Chi{I}\left(  x \right)\Chi{J}\left( y \right) \Chi{K}\left( x c^{-1}(y) \right) }   
  \end{split}
\end{equation*}
Let us denote by $C^{-1}(K)$ the indices $C^{-1}(K) = \{ j : \exists i \in K, c(i) = j \}$. Then we get that:  
\begin{equation*}
  \begin{split}
    \Chi{K}\left( x c^{-1}(y) \right) = \prod_{i \in K}{x_{i}y_{c_{i}}} =  \Chi{K}\left( K \right) \Chi{C^{-1}(K)}\left( y  \right)
  \end{split}
\end{equation*} Recall that for any $I,J \subset [n]$ it holds that: 
\begin{equation*}
  \begin{split}
    \expp{ \Chi{I}(x)\Chi{J}(x) } = \expp{ \Chi{I \Delta J}(x) } = \mathbf{1}_{ I = J }
  \end{split}
\end{equation*}
And therefore the above can be simplified into: 
\begin{equation*}
  \begin{split}
    \sum_{|I| \text{is odd} }{\alpha_{I}^{2}\beta_{C^{-1}(I)}  \left( 1 -2 \varepsilon \right)^{|I|}}   
  \end{split}
\end{equation*}

\subsection{2.c} 
First let's bound from below the expectation by the given that $f$ and $g$ pass the test with probability at least $\frac{1}{2} + \delta$:

\begin{equation*}
  \begin{split}
    & \expp{\mu f\left( \mu x \right) \cdot g\left( y \right)f\left(z \cdot x c^{-1}\left( y \right)  \right)} \\ 
    & = \prb{\mu f\left( \mu x \right) \cdot g\left( y \right) = f\left(z \cdot x c^{-1}\left( y \right)  \right)} -  \prb{\mu f\left( \mu x \right) \cdot g\left( y \right) \neq f\left(z \cdot x c^{-1}\left( y \right)  \right)} \\ 
   & \ge \frac{1}{2}+\delta - \left( \frac{1}{2} - \delta \right) = 2\delta
  \end{split}
\end{equation*}
Thus in total the inequality of the above section becomes: 
 \begin{equation*}
   \begin{split}
     \sum_{|I| \text{is odd} }{\alpha_{I}^{2}\beta_{C^{-1}(I)}  \left( 1 -2 \varepsilon \right)^{|I|}} \ge 2\delta 
   \end{split}
 \end{equation*}
Using Cauchy-Schwartz to bound from above, we obtain: 
\begin{equation*}
   \begin{split}
     4\delta^{2} \le & \left( \sum_{|I| \text{is odd} }{\alpha_{I}^{2}\beta_{C^{-1}(I)}  \left( 1 -2 \varepsilon \right)^{|I|}} \right)^{2}  \le  \sum_{|I| \text{is odd} }{ \alpha_{I}^{2}  } \cdot  \sum_{|I| \text{is odd} }{\alpha_{I}^{2}\beta_{C^{-1}(I)}^{2}  \left( 1 -2 \varepsilon \right)^{2|I|}} \\ 
    \le  &  \sum_{|I| \text{is odd} }{\alpha_{I}^{2}\beta_{C^{-1}(I)}^{2}  \left( 1 -2 \varepsilon \right)^{2|I|}} 
   \end{split}
 \end{equation*}
 Now let's denote by $\eta \in \left( 0,1 \right)$ a threshold parameter and separate the above summation into two part, when the first part sums up the elements in which $|I| \le \eta n$ and the second sums elements in which $|I| \ge \eta n$: 
 \begin{equation*}
   \begin{split}
     4\delta^{2} \le &  \sum_{|I| \text{is odd} , |I| \le \eta n  }{\alpha_{I}^{2}\beta_{C^{-1}(I)}^{2}  \left( 1 -2 \varepsilon \right)^{2|I|}} + \sum_{|I| \text{is odd} , |I| \ge \eta n  }{\alpha_{I}^{2}\beta_{C^{-1}(I)}^{2}  \left( 1 -2 \varepsilon \right)^{2|I|}} \\ 
     \le &  \sum_{|I| \text{is odd} , |I| \le \eta n  }{\alpha_{I}^{2}\beta_{C^{-1}(I)}^{2}  \left( 1 -2 \varepsilon \right)^{2|I|}} + \left( 1 -2 \varepsilon \right)^{2\eta n }\sum_{|I| \text{is odd} , |I| \ge \eta n  }{\alpha_{I}^{2}\beta_{C^{-1}(I)}^{2}  } \\ 
\le &  \sum_{|I| \text{is odd} , |I| \le \eta n  }{\alpha_{I}^{2}\beta_{C^{-1}(I)}^{2}} + \left( 1 -2 \varepsilon \right)^{2\eta n }\sum_{|I| \text{is odd} , |I| \ge \eta n  }{\alpha_{I}^{2}\beta_{C^{-1}(I)}^{2}  } 
   \end{split}
 \end{equation*}
 When in the last transition we use the fact that $1-2\varepsilon < 1$. By picking $\eta$ such that $(1-2\varepsilon)^{2\eta n } = \Theta\left( \delta^{3} \right)$ we have that for a family of tests:   
 \begin{equation}
   \label{equ:we}
   \begin{split}
     3\delta^{2} \le  \sum_{|I| \text{is odd} , |I| \le \eta n  }{\alpha_{I}^{2}\beta_{C^{-1}(I)}^{2}}
   \end{split}
 \end{equation}
 As the summation is over $I$ at odd size, the empty set is not counted in the summation, namely there must be a non empty $I$ such that $|I| \le \frac{1}{2} \log_{1-2\varepsilon}\left( \delta^{3} \right)$ and $\alpha_{I}\beta_{C^{-1}\left( I \right)}$ have non zero weight. Thus we can define: 
 \begin{equation*}
   \begin{split}
 L_{f} & = \left\{ I : |I| \le \frac{1}{2} \log_{1-2\varepsilon}\left( \delta^{3} \right) \text{ and } |I| \text{ is odd } \right\} \\
 M_{g} & = \left\{ C^{-1}(I) : |I| \le \frac{1}{2} \log_{1-2\varepsilon}\left( \delta^{3} \right) \text{ and } |I| \text{ is odd } \right\}
   \end{split}
 \end{equation*}
%\end{proof}
%
\subsection{Ex 3. The label cover problem.} 
\paragraph{The reduction.} Let $\braket{G=(V,E), \{ c_{e} \} }$ be a given instance of the Label cover problem. For each edge $e = \{v,u\} \in E$ define the test $T_{\varepsilon}\left( c_{e} \right)$ as defined above , Thus in total we define a $|E|$ tests, denote them by $T$. Consider the language $L$ such that a test collection $T$ is in $L$ if there exists function $f \times V$ such that the probability:  
\begin{equation*}
  \begin{split}
    \prb{T_{\varepsilon}(c_{ \{ v,u\} }) \text{ accepts on } f_{v},f_{u} } \ge \frac{1}{2} + \delta
  \end{split}
\end{equation*}
For every $\{v, u\} \in E$. A probabilistic verifier takes a candidate $f\times V : \pm  \times V \rightarrow \pm$, picks a random edge $e \in E$ and then check $T_{\varepsilon}\left( c_{e} \right)$ over the functions $f_{v},f_{u}$.    


%And notice that on binary field the test is equivalence for picking a random equation from the liner system in which each equation contains exactly tree variables.  
%vertex $v \in V$ define a variable $\xi_{v} \in \mathbb{F}_{2}^{|\Sigma|}$ and for any when we think on the functions $f,g : \pm^{|\Sigma|} \rightarrow \pm $ as the 'witness'/'input' given by the 'prover'/'user'
%For any $x,y,\mu, z$ 
\paragraph{Completeness. } Suppose that $\braket{G=(V,E), \{ c_{e} \} } \in (\mu,1)$-Label Cover then either there exists a labeling $A$ such that $c_{vu}(A(v)) = A_{u}$ for any $\{v,u\}\in E$ or that any labeling satisfies at most $\mu$ constraints. For completeness let's assume the first case, and denote by $A$ the satisfying labeling. Consider the function $f \times V : \pm \times V \rightarrow \pm$ defined as follow: $f_{v} = \Chi{A(v)}$, So by the first section of part 2 we have that any of the test accepts with probability $1-\varepsilon$. That it, as we pick a test uniformly random, the existences of satisfying labeling for the label cover problem give a function that pass the test with probability $1-\varepsilon$. 
%\begin{equation*}
%  \begin{split}
%    & A(v) = \Chi{i} \\ 
%    & A(u) = \Chi{j} \\ 
%    & j = P_{uv} i  
%  \end{split}
%\end{equation*}
\paragraph{Soundness.} Now, assume the second case, namely that any labeling satisfies at most $\mu$ constraints. Also assume through contradiction that there exists an assignment that satisfies more than $\frac{1}{2}+\delta$ equations, so by the same arguments we use in section 2.b we have that the expectation of the product $ \expp{\mu f_{v}(\mu x )f_{u}(y)f_{v}(zx c^{-1}(y))} \ge 2 \delta$ when here, in addition for taking the expectation over the $x,y,z,\mu$ we also summing on the edges $\{v,u\}\in E$. 

Now we are about to show that for at least $\delta$ of tests the product $ \expp{\mu f_{v}(\mu x )f_{u}(y)f_{v}(zx c^{-1}(y)) | u,v }$ conditioned on the test is greater than $\delta$. For convenient let's use the notation $\expp{ \cdot } \ge \delta$ for referring to tests that the averaging in on their product is grater than $\delta$, and by the same manner let's use the notation $\expp{\cdot } \le \delta$. So:   

  \begin{equation*}
    \begin{split}
      2\delta \le  &  \expp{\mu f_{v}(\mu x )f_{u}(y)f_{v}(zx c^{-1}(y))} = \\ 
    & \  \prb{ u,v \text{ s.t } \expp{ \cdot } \ge \delta }  \expp{\mu f_{v}(\mu x )f_{u}(y)f_{v}(zx c^{-1}(y)) | u,v \text{ s.t } \expp{ \cdot  } \ge \delta  } +\\ 
  & \  \prb{ u,v \text{ s.t } \expp{\cdot }\le \delta } \expp{\mu f_{v}(\mu x )f_{u}(y)f_{v}(zx c^{-1}(y)) | u,v \text{ s.t } \expp{\cdot } \le \delta } \\
& \le \prb{ u,v \text{ s.t } \expp{ \cdot } \ge \delta} \cdot 1 + \prb{ u,v \text{ s.t } \expp{\cdot } \le \delta}  \cdot \delta \\ 
            & \le \prb{ u,v \text{ s.t } \expp{\cdot }\ge \delta}  + \delta 
    \end{split}
  \end{equation*}
  Thus for at least $\delta$ fraction of the tests equation \ref{equ:we} holds. Now consider the follow probabilistic assignment, for any vertex $v$ we choose a set $I \subset [n]$ at probability that equals to the projection of $f_{v}$ on $\Chi{I}$ square, namely $|\braket{f_{v},\Chi{I} }|^{2}$ then picking uniformly form the support of $I$ a label for $v$. Therefore for any tests associate with $u,v$ satisfies $\expp{\cdot} \ge \delta$ we have that the probability that $c_{v,u}A(v)=A(u)$ is at least:     
  \begin{equation*}
    \begin{split}
      & \sum_{|I| \text{is odd} , |I| \le \eta n  }{|\braket{f_{v},\Chi{I} }|^{2}|\braket{f_{ru},\Chi{c^{-1}(I)} }|^{2}  \cdot \prb{\text{ pick } i \in I, j\in C^{-1}(I) , c(i)=j }  } \\ 
      & \ge \sum_{|I| \text{is odd} , |I| \le \eta n  }{|\braket{f_{v},\Chi{I} }|^{2}|\braket{f_{ru},\Chi{c^{-1}(I)} }|^{2} \cdot \frac{1}{|I||C^{-1}(I)|}} \\ 
      & \ge \left(\frac{1}{2} \log_{1-2\varepsilon}\left( \delta^{3} \right)\right)^{-2}\cdot \sum_{|I| \text{is odd} , |I| \le \eta n  }{|\braket{f_{v},\Chi{I} }|^{2}|\braket{f_{ru},\Chi{c^{-1}(I)} }|^{2}} \\
      & \ge \left(\frac{1}{2} \log_{1-2\varepsilon}\left( \delta^{3} \right)\right)^{-2}\cdot 3\left( \frac{\delta}{2} \right)^{2} 
    \end{split}
  \end{equation*}
  Thus in total the labeling satisfies $\delta \cdot \left(\frac{1}{2} \log_{1-2\varepsilon}\left( \delta^{3} \right)\right)^{-2}\cdot 3\left( \frac{\delta}{2} \right)^{2} $ of the constraints. That it, setting that number to $\eta$ obtains the requested.   
\section{Part 3.}

\paragraph{Label cover when the aleph-bet depends on the vertex.} Instead of showing reduction into the general label cover we will show a reduction to a similar problem in which vertices can have an additional restriction on the valid charters that one can sets on. In formal, we will say that $\braket{G, \{ \Sigma_{v} : v \in V \}, \{c_{e} : e \in E \}} $ instance of Generalized-Label-Cover if there is an labeling $A : V \rightarrow \Sigma$ such that for any $\{v,u\} \in E$ it holds that $c_{e}A(v) = A(u)$ and in addition for any $v \in V$ we have that $A(v) \in \Sigma_{v} \subset \Sigma$.  

\paragraph{The reduction.} Define the Bipartite graph $G = (L,R,E)$. Associate the left vertices with the variables and the right with the closures. Define $\{u,v\}$ to be an edge if the literal which associate with the vertex $u$ is in the closure associate with vertex $v$. For the alphabet take $\Sigma = \mathbb{Z}_{2}^3$. For any right vertex $v\in R$ define $\Sigma_{v}$ be all the assignments for which the $v$-closures is satisfied and for any left vertex $u$ define $\Sigma_{u} = \{ \left( 1 , 0 , 0 \right), \left( 0 , 0, 0 \right) \}$. Finally define $c_{e}$ for $ e = \left\{ v \in R, u \in L \right\}$ to be the projection of $\sigma \in \Sigma$, setted on $v$, to the coordinate corresponding with $u$. For example, assume that $v$ associate with $x\vee y \vee z $ and let $u$ be the vertex associate with $x$, And assume that $A(v) = (1,0,1)$ , then $c_{e}A_{v} = (1,0,0)$.           

%For the right vertices, consider the $3!$ permutations of the closures, and associate for any of them a right vertex. For example, consider the closure $ x\vee y \vee z$, then associate a right vertex with the closures $x \vee y \vee z$,  $y \vee z \vee x$, $z \vee x \vee y$, $y \vee x \vee z$ and etc.

%For the edges, connect between any closure on the right an edge between it and the left vertices which associate with the first terminal on the closure.Notice that the right degree of the graph is $1$ while the left degree of the vertex associated with terminal $x$ is:
%\begin{equation*}
%  \begin{split}
%     2 \times \text{ number of closures containing } x 
%  \end{split}
%\end{equation*}
%The permeations on the edges defined as follow: Let $x$ be variable, and $\psi$ a closure in which $x$ is the first terminal. Sets the permeation of the edges as following, if $x$ appears in his positive form, namely $\psi = x \vee .. $ then set on the edge $\left\{ x, \psi \right\}$ the identity permutation otherwise set the flipping permutation. 
%

\paragraph{Completeness. } Suppose that $\varphi \in \text{E3-CNF-SAT}$ and let $x \in \mathbb{F}_{2}^{*}$ be the assignment that satisfies $\varphi$. That it, $\varphi\left( x \right) = \mathbf{True}$. Let $A$ be the labeling that sets for any vertex on the left the bit matched to that literal by $x$ follows by zeros padding. And for any right vertex the triple of the bits corresponding to literals involving in the associated closure. By the fact that $x$ satisfies $\varphi$ any closure in $\varphi$ is satisfied by $x$ and therefore each of the right vertices (closures) see on his local view a character of $\Sigma_{v}$. In addition by the definition of the construction any pair of connected vertices satisfies the edge restriction. 

\paragraph{Soundness.} Suppose that $\varphi \in \text{E3-CNF-SAT}$ but not satisfiable and $\braket{G, \{ \Sigma_{v} : v \in V \}, \{c_{e} : e \in E \}} $ is an instance obtained by the reduction above. Assume towards contradiction that there exists labeling $A$ such that more than $\mu^{\prime}= 6\mu$ of the restriction $\{c_{e}\}$ are satisfied. 

Define by $\alpha_{i}$ to be the number of right vertices which satisfy exactly $i$ edges, that it, 
\begin{equation*}
  \begin{split}
    \alpha_{i} = \left| \left\{ | \left\{ c_{e}A\left( v \right) = A\left( u \right) : u \in L \right\} | = i : v \in R   \right\} \right| 
  \end{split}
\end{equation*}

\begin{claim}
  For any labeling $A$ such that $\alpha_{3} \ge \mu$ there exists an assignment $x \in \mathbb{F}_{2}^{*}$ satisfies at least $\mu$ portion of the restrictions. 
\end{claim}

\begin{proof}
  The proof is trivial. 
\end{proof}

\begin{claim}
  For any labeling $A$ that satisfy $\xi$ constraints, there exists labeling $A^{\prime}$ such that any constraint that satisfied by $A$ also satisfied by $A^{\prime}$ and in addition $\alpha_{0}=\alpha_{1}=0$. Put it differently, we can assume that $\alpha_{0}=\alpha_{1}=0$.
\end{claim}

\begin{proof}
  Let $v \in R$ be a vertex that satisfies less than two edges. Recall that $\Sigma_{v}$ contains all the triple that satisfy the closure associated with $v$. By the fact that for any 3-CNF closure there is exactly one assignment which does not satisfy it, It follows that $|\Sigma_{v}| = 2^{3}-1 = 7 \ge 2^{2}$. Therefore, we can replace $A(v)$ by a triple that agree with the first two vertices connected to it.     
\end{proof}

Using the above claim we can infer that $\alpha_{2} + \alpha_{3} = |R|$ and in addition $2\cdot \alpha_{2} + 3 \cdot \alpha_{3} \ge \mu^{\prime} \cdot 3|R|$. Thus, $\alpha_{3}\ge \left( 3\mu^{\prime} - 2 \right)|R|$. 
Particularly if $\mu^{\prime} \ge \frac{\mu + 2}{3}$ then $\alpha_{3} \ge \mu|R|$, Combining the claim above we get a contradiction to the fact that $\varphi \in\left( \mu,1 \right)$ gap-3E-CNF-SAT and not satisfiable. 

%\begin{claim}
%  Let $A$ an assignment such that satisfy $\xi$ relations and it's support on the right side is at most $1 - \alpha$ then flipping assignment $ 1 + A$ also satisfy $\xi$ relations, but has at least $\alpha$ support on the right side.     
%\end{claim}
%\begin{proof}
%\end{proof}
%
%Suppose that we have an assignment that satisfies $ \ge \mu/3 $ of the closers,  


\end{document}



\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={7.5in, 10in} ]{geometry}
\usepackage{braket}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{patterns, shapes.arrows}
\usepackage{adjustbox}
\usepackage{tikz-network}
\usepackage[linesnumbered]{algorithm2e}
\usepackage{multicol}
\usepackage[backend=biber,style=alphabetic,sorting=ynt]{biblatex}
\usepackage{xcolor}
\usepackage{pgfplots}
\DeclareUnicodeCharacter{2212}{âˆ’}
\usepgfplotslibrary{groupplots,dateplot}
\pgfplotsset{compat=newest}

\addbibresource{sample.bib} %Import the bibliography file

\newcommand{\commentt}[1]{\textcolor{blue}{ \textbf{[COMMENT]} #1}}
\newcommand{\ctt}[1]{\commentt{#1}}
\newcommand{\prb}[1]{ \mathbf{Pr} \left[ {#1} \right]}
\newcommand{\expp}[1]{ \mathbf{E} \left[ {#1} \right]}
\newcommand{\onotation}[1]{\(\mathcal{O} \left( {#1}  \right) \)}
\newcommand{\ona}[1]{\onotation{#1}}
\newcommand{\PSI}{{\ket{\psi}}}
\newcommand{\LESn}{\ket{\psi_n}}
\newcommand{\LESa}{\ket{\phi_n}}
\newcommand{\LESs}{\frac{1}{\sqrt{n}}\sum_{i}{\ket{\left(0^{i}10^{n-i}\right)^{n}}}}
\newcommand{\Hn}{\mathcal{H}_{n}}
\newcommand{\Ep}{\frac{1}{\sqrt{2^n}}\sum^{2^n}_{x}{ \ket{xx}}}
\newcommand{\HON}{\ket{\psi_{\text{honest}}}}
\newcommand{\Lemma}{\paragraph{Lemma.}}
\newcommand{\PonB}{ \rho + \frac{5}{16}\delta\le \frac{3}{4} + \frac{1}{16} } 
\newcommand{\Cpa}{[n, \rho n, \delta n]}
%\setlength{\columnsep}{0.6cm}
\newcommand{\Jvv}{ \bar{J_{v}} } 
\newcommand{\Cvv}{ \tilde{C_{v}} } 

\newcommand{\Gz}{ G_{z}^{\delta} } 
\newcommand{ \Tann } {  \mathcal{T}\left( G, C_0 \right) }
\begin{document}



\newcommand{\dalg}[1]{\expp{#1 : \text{alg} \sim \tilde{\text{alg}}}}
\newcommand{\dsig}[1]{\expp{#1 : \sigma \sim \tilde{\sigma}}}
\newcommand{\calg}{c_{\text{alg}}}
\newcommand{\cbase}{c_{\text{base}}}


\title{Online Computation, Ex 2. } 
\author{David Ponarovsky}
\maketitle
%\abstract{We propose a new simple construction based on Tanner Codes, which yields a good LDPC testable code.} 

\begin{multicols*}{2}
  \paragraph{ex1.} Find a simple description of the work-function algorithm in the case of uniform metric space. 
  \paragraph{Solution.} Recall that in the work-function algorithm we weight the configutations by the price one has pay for serving all the requests and ending at those configuration combing the distance between them and the current configuration. In the paging problem the configurations are the content of the cache stack.  

\begin{algorithm}[H]
  \caption{Work-function-Algo for paging.}
    \label{alg:three}
    %\KwData{ $x \in \mathbb{F}_{2}^{n}$ }
    %\KwResult{ $\arg\min {\left\{  y \in C : |y + x|  \right\} }$ if $d(y,C) < \tau $ and False otherwise. }
    %$ L \leftarrow \text{Array} \{ \} $\\
    \For { each new day $j$  } {
      \If{ $x < 1$  } { 
	$ \xi_{j} \Leftarrow 1 - x$ \\
	$ x \Leftarrow \left( 1 + \frac{1}{M} \right) x + \frac{1}{\left( c - 1  \right) M } $ \\
	$ z_{j} \Leftarrow 1$
      }
    }
  \end{algorithm}
  \paragraph{ex2.} Consider the following $3$-point metric space, $w\left( a,b \right) = 1 $ and $w\left( \cdot, c  \right) = M $. The initial configuration is $ \left\{ b,c \right\}$ ($2$ servers). Show that randomized competitive ratio, for some value of $M$ is $ > H_{2} = 1 + \frac{1}{2}$. 
  \paragraph{Solution.} Define the following distribution: 
  \begin{equation*}
    \begin{split}
      \tilde{\sigma} = \begin{cases}
	(ab)^{\frac{M}{3}} & \text{w.p  } \frac{1}{2} \\ 
	(ab)^{\frac{M^{100}}{3}} &  \text{w.p  } \frac{1}{2}  
      \end{cases}
    \end{split}
  \end{equation*}
  Using Yao's principle, it's enough to show that any deterministic algorithm is $H_{2}$ competitive in expectation against that specific distribution. First, notice that knowing what is the exactly drawn $\sigma$, fixes an optimal strategy which is one of the following: moving the server initialized at $a$ between $a,b$ points alternately or choosing first the server that is located in $c$ into $a$ in the second scenario.
  Putting down, we obtain that: 
  \begin{equation*}
    \begin{split}
      \dsig{ \cbase \left( \sigma \right)  } = \frac{1}{2} \left(  \frac{M}{3} + M  \right) = \frac{5}{6}M 
    \end{split}
  \end{equation*}
  Meanwhile, by the fact that reading any prefix of requests series at length less than $\frac{M}{3}$ doesn't expose any information about the drawn input which wasn't known at the initialized moment, it follows by indistinguishable arguments that the best a randomized algorithm can do is to guess.     
  \paragraph{ex3.} Show that randomized marking algortihm cannot be $c$-competitive against the adaptive online adversary, for $c=o\left( k \right)$. 
  \paragraph{Solution.} Assume by contrdiction that there is a constant $c> 1$, and a randomizded algorithm which is an $c$-competitive in the adadptive online setting. According to the theroem shown at class, If there exists an $\alpha$ competitive alg for an online problem in the non-adaptive setting and inadttion there exists a $\beta$ competitive algorithm for the same problem aginst adaptive online adversary, then it holds that there exists an algorithm which is $\alpha\beta$ competitive against an offline adaptive adversary. 
  Combining the fact that randomized can't help against such an adversary, we obtain that the deterministic competitive ratio is lower than $\alpha\beta$. As we know that a $k$-lowerbound for the deterministic regime and also a $\log k$ solution using randomization against a non-adaptive adversary, we obtain that      
  \begin{equation*}
    \begin{split}
      & \alpha\beta \ge k \\ 
      \Rightarrow  & \frac{k}{c}\log k \ge k 
    \end{split}
  \end{equation*} But for any $ k \le \log 2^{c} $ we obtain the oppsite direction. This means that there is a range of valid $k$ that obtains a better ratio than the lower bound. And that is a contradiction.   

  \paragraph{ex4 - Ski Rental.} At each step, the adversary decides either to continue or stop. Stop terminating the game. If it continues, the online algorithm decides either to rent or buy. Rent costs $1$. Buy costs $M > 1$. Design a primal-dual randomized online ski-rental algorithm with a better than $2$ competitive ratio.  
  \paragraph{Solution.}. Let's start by formulating an integer LP for the Ski-Rental problem. Denote by $m$ the days' number, and associate a variable $x$, indicating whether the algorithm decides to buy. Also, let's associate a variable $\xi_{j}$ for each day which indicates if the algorithm pays for rent. In each turn, the solution must satisfy the restrictions $ \xi_{j} + x  \ge 1$. The cost which we would like to minimize is $ M\cdot x + \sum_{j}{\xi_{j}}$. So, in overall, we get that LP is:    
  \begin{equation*}
    \begin{split}
      & \min{ Mx + \sum_{j}{\xi_{j}}} \\
      \text{s.b } & x + \xi_{j} \ge 1 \Leftrightarrow \\ 
      & \begin{bmatrix}
	1 & 1 & 0 & 0 & \cdot\\
	1 & 0 & 1 & 0 & \cdot\\
	1 & 0 & 0 & 1 & \cdot\\
	1 & 0 & 0 & 0 & \cdot\\
	\cdot & \cdot & \cdot & \cdot & \cdot
      \end{bmatrix} 
      \begin{bmatrix}
	x \\
	\xi_{1} \\ 
	\xi_{2} \\
	\xi_{3} \\
	\cdot 
      \end{bmatrix} \ge
      \begin{bmatrix}
	1 \\
	1 \\ 
	1 \\
	1 \\
	\cdot 
      \end{bmatrix}
    \end{split}
  \end{equation*}
  So the dual program is 
  \begin{equation*}
    \begin{split}
      & \max{ \sum_{j}{z_{j}}} \\
      \text{subject } &  \text{to}  \\ 
      & \begin{bmatrix}
	1 & 1 & 1 & 1 & \cdot \\
	1 & 0 & 0 & 0 & \cdot \\
	0 & 1 & 0 & 0 & \cdot \\
	0 & 0 & 1 & 0 & \cdot \\
	\cdot & \cdot & \cdot & \cdot & \cdot
      \end{bmatrix} 
      \begin{bmatrix}
	x \\
	z_{1} \\ 
	z_{2} \\
	z_{3} \\
	\cdot 
      \end{bmatrix} \le
      \begin{bmatrix}
	M \\
	1 \\ 
	1 \\
	1 \\
	\cdot 
      \end{bmatrix}
    \end{split}
  \end{equation*}
  So in total.

  %\fbox {
\begin{algorithm}[H]
    \caption{Ski-Rental}
    \label{alg:three}
    %\KwData{ $x \in \mathbb{F}_{2}^{n}$ }
    %\KwResult{ $\arg\min {\left\{  y \in C : |y + x|  \right\} }$ if $d(y,C) < \tau $ and False otherwise. }
    %$ L \leftarrow \text{Array} \{ \} $\\
    \For { each new day $j$  } {
      \If{ $x < 1$  } { 
	$ \xi_{j} \Leftarrow 1 - x$ \\
	$ x \Leftarrow \left( 1 + \frac{1}{M} \right) x + \frac{1}{\left( c - 1  \right) M } $ \\
	$ z_{j} \Leftarrow 1$
      }
    }
  \end{algorithm}
%}
  \paragraph{ex5.} Prove Yao's minimax principle. 

  \begin{equation*}
    \begin{split}
      & \forall \text{rand. } \tilde{\text{alg}} \  \exists  \ \sigma  \\
      & \ \ \dalg{ \calg\left( \sigma \right) } \ge c \cdot   \cbase\left( \sigma \right) \\  
      & \Leftrightarrow \exists \ \text{rand.} \ \tilde{\sigma} \forall \text{alg} \\  
      & \ \ \dsig{  \calg\left( \sigma \right)  } \ge c \dsig{ \cbase \left( \sigma \right)  } 
    \end{split}
  \end{equation*}
  \paragraph{Solution.} First direction, assume through contradiction that there exists a deterministic algorithm such that for all distributions $\tilde{\sigma}$ :
  \begin{equation*}
    \begin{split}
      \dsig{  \calg\left( \sigma \right)  } < c \dsig{ \cbase \left( \sigma \right)  }
    \end{split}
  \end{equation*} And that holds, in paritcular, for distribution $\tilde{\sigma}$ which suportted by a single $\sigma$. Hence, because any deterministic algorithm is also a randomized algorithm, set it to be $ \tilde{\text{alg}}$, and that immediately yields a contradiction. 
  It is left to show the second direction. By the monotonic property of random variables, we have that for any distribution $\tilde{\sigma}$: 
  \begin{equation*}
    \begin{split}
      &  \dalg{ \dsig{ \calg\left( \sigma \right) } } \\
      & \ \  \ \ \ge c \cdot \dalg{ \dsig{  \cbase\left( \sigma \right) } } \\
      & \dsig{ \dalg{ \calg\left( \sigma \right) } } \\
      &  \ \  \ \ \ge c \cdot \dsig{ \dalg{  \cbase\left( \sigma \right) } } \\
      & \dsig{ \dalg{ \calg\left( \sigma \right) } } \\
      & \ \ \ \  \ge c \cdot \dsig{  \cbase\left( \sigma \right) }  
    \end{split}
  \end{equation*}
  And by the fact that inequality of exception between random variables follows an existence of atomic event on which the inequality holds, we obtain that there must exists at least a single $\sigma$ such that:
  \begin{equation*}
    \begin{split}
      \dsig{ \dalg{ \calg\left( \sigma \right) } } \ge c \cdot \dsig{  \cbase\left( \sigma \right) }  
    \end{split}
  \end{equation*}
  And that ends the proof.
\end{multicols*}
\printbibliography 
\end{document}







\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={7.5in, 10in} ]{geometry}
\usepackage{braket}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{patterns, shapes.arrows}
\usepackage{adjustbox}
\usepackage{tikz-network}
\usepackage[linesnumbered]{algorithm2e}
\usepackage{multicol}
\usepackage[backend=biber,style=alphabetic,sorting=ynt]{biblatex}
\usepackage{xcolor}
\usepackage{pgfplots}
\DeclareUnicodeCharacter{2212}{âˆ’}
\usepgfplotslibrary{groupplots,dateplot}
\pgfplotsset{compat=newest}

\addbibresource{sample.bib} %Import the bibliography file

\newcommand{\commentt}[1]{\textcolor{blue}{ \textbf{[COMMENT]} #1}}
\newcommand{\ctt}[1]{\commentt{#1}}
\newcommand{\prb}[1]{ \mathbf{Pr} \left[ {#1} \right]}
\newcommand{\expp}[1]{ \mathbf{E} \left[ {#1} \right]}
\newcommand{\onotation}[1]{\(\mathcal{O} \left( {#1}  \right) \)}
\newcommand{\ona}[1]{\onotation{#1}}
\newcommand{\PSI}{{\ket{\psi}}}
\newcommand{\LESn}{\ket{\psi_n}}
\newcommand{\LESa}{\ket{\phi_n}}
\newcommand{\LESs}{\frac{1}{\sqrt{n}}\sum_{i}{\ket{\left(0^{i}10^{n-i}\right)^{n}}}}
\newcommand{\Hn}{\mathcal{H}_{n}}
\newcommand{\Ep}{\frac{1}{\sqrt{2^n}}\sum^{2^n}_{x}{ \ket{xx}}}
\newcommand{\HON}{\ket{\psi_{\text{honest}}}}
\newcommand{\Lemma}{\paragraph{Lemma.}}
\newcommand{\PonB}{ \rho + \frac{5}{16}\delta\le \frac{3}{4} + \frac{1}{16} } 
\newcommand{\Cpa}{[n, \rho n, \delta n]}
%\setlength{\columnsep}{0.6cm}
\newcommand{\Jvv}{ \bar{J_{v}} } 
\newcommand{\Cvv}{ \tilde{C_{v}} } 

\newcommand{\Gz}{ G_{z}^{\delta} } 
\newcommand{ \Tann } {  \mathcal{T}\left( G, C_0 \right) }
\begin{document}



\newcommand{\dalg}[1]{\expp{#1 : \text{alg} \sim \tilde{\text{alg}}}}
\newcommand{\dsig}[1]{\expp{#1 : \sigma \sim \tilde{\sigma}}}
\newcommand{\calg}{c_{\text{alg}}}
\newcommand{\cbase}{c_{\text{base}}}


\title{Online Computation, Ex 2. } 
\author{David Ponarovsky}
\maketitle
%\abstract{We propose a new simple construction based on Tanner Codes, which yields a good LDPC testable code.} 

\begin{multicols*}{2}
  \paragraph{ex1.} Find a simple description of the work-function algorithm in the case of uniform metric space. 
  \paragraph{Solution.} Recall that in the work-function algorithm, we weigh the configurations by the price one has paid for serving all the requests and end at those configurations, combing the distance between them and the current configuration. In the paging problem, the configurations are the content of the cache stack.  
  Denote by $m$ and $k$ the hard disk size and the number of servers. Enumarate each of the valid stack states by $Q = q_{0},q_{1},q_{2},\dots q_{M}$  where $M = \binom{m}{k}$. In addition, define a weight function $w : Q \rightarrow \mathbb{R}$ to be Hamming distance between pair of configurations. Finally, define a $M \times m$ table $W_{i,j}$ to store the optimal work that has to be done while serving $\sigma_{0},\sigma_{1},\sigma_{2},\dots \sigma_{j}$ requests and ending in configuration $q_{i}$.     

  \paragraph{Cliam.} In the uniform case, if $v_{1},v_{2},v_{3},..,v_{l}$ is an configuration path that servers $\sigma_{1},\sigma_{2}..\sigma_{l}$ and ends at configuration $v_{l}$ such that $v_{l}$ deffers from $v_{l-1}$ by $t > 1$ elements, then there is also an optimal path $v_{1},u_{2}..u_{l-1}v_{l}$ which also serves the requests and $v_{l}$ differs from $u_{l-1}$ by at most $t-1$ elements. 
  \paragraph{Proof.} As $v_{l}$ differs from $v_{l-1}$ by at least two elements, there must be an element $\tilde{\sigma} \neq \sigma_{l-1}$ belongs to $v_{l-1}$, By the assumption that the algorithm always holds a full cache, there must be an element $\sigma^{\star} \in v_{l} / v_{l-1}$ (that could be $\sigma_{l}$ but doesn't has to). Consider the configuration $u_{l-1} = v_{l-1} \cup \sigma^{\star} / \tilde{\sigma}$. Since $v_{1}v_{2},..,v_{l-1},v_{l}$ is optimal, $v_{1},v_{2}..,v_{l-2}u_{l-1}$ must also be optimal, and cost at most $1$ more than the  $v_{1}v_{2},..,v_{l-1} $, clearly we have that $v_{1},v_{2},..u_{l-1}v_{l}$ is also optimal and we done. 

  By induction argument, we obtain that in the uniform matric, the case is enough to consider configurations paths in which any adjacent configuration pair differ by at most one element. So our work-function algorithm will take advantage of that fact to compute $W$.

  \begin{algorithm}[H]
    \caption{Work-function-Algo for paging.}
    \label{alg:three}
    $q_{0} \leftarrow $ initial state \\ 
    \For { each new request $\sigma_{j}$  } {
      \For { $ i \in [m] $ and $ q_{i} \neq q_{0}$ } {
	\If { $ \sigma_{j} \in q_{i}$ } {
	  \For { $\left( \sigma^{\star}, \tilde{\sigma} \right)$ such that $ \sigma^{\star} \in q_{i}$ and   $ \tilde{\sigma} \notin q_{i}$ } {
	    $q_{i^{\prime} } \leftarrow  q_{i} / \sigma^{\star} \cup \tilde{\sigma}$ \\ 
	    $W_{i,j} \leftarrow \min \left\{   W_{i,j} ,  W_{i^{\prime},j-1} + 1 \right\} $
	  }
	  $W_{i,j} \leftarrow \min \left\{   W_{i,j} ,  W_{i,j-1}  \right\} $ \\
	}
	\Else {
	  $W_{i,j} \leftarrow \infty $ 
	}
      }
      $q_{0} \leftarrow \arg \min_{i : \sigma_{j} \in q_{i}} \left\{ W_{i,j-1} +  \mathbf{Ham}\left( q_{i}, q_{0} \right)  \right\} $ \\
      Set $q_{0}$ as the current state, evict and serve if needed. 
    }
  \end{algorithm}
  \paragraph{ex2.} Consider the following $3$-point metric space, $w\left( a,b \right) = 1 $ and $w\left( \cdot, c  \right) = M $. The initial configuration is $ \left\{ b,c \right\}$ ($2$ servers). Show that randomized competitive ratio, for some value of $M$ is $ > H_{2} = 1 + \frac{1}{2}$. 
  \paragraph{Solution.} Define the following distribution: 
  \begin{equation*}
    \begin{split}
      \tilde{\sigma} = \begin{cases}
	(ab)^{\frac{M}{3}} & \text{w.p  } \frac{1}{2} \\ 
	(ab)^{\frac{M^{100}}{3}} &  \text{w.p  } \frac{1}{2}  
      \end{cases}
    \end{split}
  \end{equation*}
  Using Yao's principle, it's enough to show that any deterministic algorithm is $H_{2}$ competitive in expectation against that specific distribution. First, notice that knowing what is the exactly drawn $\sigma$, fixes an optimal strategy which is one of the following: moving the server initialized at $a$ between $a,b$ points alternately or choosing first the server that is located in $c$ into $a$ in the second scenario.
  Putting down, we obtain that: 
  \begin{equation*}
    \begin{split}
      \dsig{ \cbase \left( \sigma \right)  } = \frac{1}{2} \left(  \frac{M}{3} + M  \right) = \frac{4}{6}M 
    \end{split}
  \end{equation*}
  Meanwhile, by the fact that reading any prefix of requests series at length less than $\frac{M}{3}$ doesn't expose any information about the drawn input which wasn't known at the initialized moment, it follows by indistinguishable arguments that the best a randomized algorithm can do is to guess. (Formal proof is a reduction from an algorithm that decides at an arbitrary step to one that decides at the first turn). Furthermore, we could assume that any of the deterministic algorithms which we test $\tilde{\sigma}$ against are aware they compete against drawn from $\tilde{\sigma}$, (As any other algorithm could only play worse than them). To conclude, we will test $\tilde{\sigma}$ against deterministic algorithms, which have to decide at the first turn in what cases there are; If they succeed, they pay OPT; Otherwise, they pay OPT $ + M$ at least. 

  So the excpection of such algorithm is at least: $ \frac{1}{2} $ OPT + $ \frac{1}{2} $ ( OPT $+ M$ ). Namly, OPT + $\frac{1}{2} M \Rightarrow $ the comppetive ratio is greater than $ 1\frac{1}{2}  $  and that is what we exactly want to prove.

  \paragraph{ex3.} Show that randomized marking algortihm cannot be $c$-competitive against the adaptive online adversary, for $c=o\left( k \right)$. 
  \paragraph{Solution.} Assume by contrdiction that there is a constant $c> 1$, and a randomizded algorithm which is an $c$-competitive in the adadptive online setting. According to the theorem shown in class, If there exists an $\alpha$ competitive algorithm for an online problem in the non-adaptive setting, and in addition, there exists a $\beta$ competitive algorithm for the same problem against an adaptive online adversary. Then it follows the existence of algorithm $\alpha\beta$ competitive against an offline adaptive adversary. 
  Combining the fact that randomized can't help against such an adversary, we obtain that the deterministic competitive ratio is lower than $\alpha\beta$. As we know that a $k$-lowerbound for the deterministic regime and also a $\log k$ solution using randomization against a non-adaptive adversary, we obtain that      
  \begin{equation*}
    \begin{split}
      & \alpha\beta \ge k \\ 
      \Rightarrow  & \frac{k}{c}\log k \ge k 
    \end{split}
  \end{equation*} But for any $ k \le \log 2^{c} $ we obtain the oppsite direction. This means that there is a range of valid $k$ that obtains a better ratio than the lower bound. And that is a contradiction.   

  \paragraph{ex4 - Ski Rental.} At each step, the adversary decides either to continue or stop. Stop terminating the game. If it continues, the online algorithm decides either to rent or buy. Rent costs $1$. Buy costs $M > 1$. Design a primal-dual randomized online ski-rental algorithm with a better than $2$ competitive ratio.  
  \paragraph{Solution.}. Let's start by formulating an integer LP for the Ski-Rental problem. Denote by $m$ the days' number, and associate a variable $x$, indicating whether the algorithm decides to buy. Also, let's associate a variable $\xi_{j}$ for each day, indicating if the algorithm pays for rent. In each turn, the solution must satisfy the restrictions $ \xi_{j} + x  \ge 1$. The cost which we would like to minimize is $ M\cdot x + \sum_{j}{\xi_{j}}$. So, in overall, we get that LP is:    
  \begin{equation*}
    \begin{split}
      & \min{ Mx + \sum_{j}{\xi_{j}}} \\
      \text{s.b } & x + \xi_{j} \ge 1 \Leftrightarrow \\ 
      & \begin{bmatrix}
	1 & 1 & 0 & 0 & \cdot\\
	1 & 0 & 1 & 0 & \cdot\\
	1 & 0 & 0 & 1 & \cdot\\
	1 & 0 & 0 & 0 & \cdot\\
	\cdot & \cdot & \cdot & \cdot & \cdot
      \end{bmatrix} 
      \begin{bmatrix}
	x \\
	\xi_{1} \\ 
	\xi_{2} \\
	\xi_{3} \\
	\cdot 
      \end{bmatrix} \ge
      \begin{bmatrix}
	1 \\
	1 \\ 
	1 \\
	1 \\
	\cdot 
      \end{bmatrix}
    \end{split}
  \end{equation*}
  So the dual program is 
  \begin{equation*}
    \begin{split}
      & \max{ \sum_{j}{z_{j}}} \\
      \text{subject } &  \text{to}  \\ 
      & \begin{bmatrix}
	1 & 1 & 1 & 1 & \cdot \\
	1 & 0 & 0 & 0 & \cdot \\
	0 & 1 & 0 & 0 & \cdot \\
	0 & 0 & 1 & 0 & \cdot \\
	\cdot & \cdot & \cdot & \cdot & \cdot
      \end{bmatrix} 
      \begin{bmatrix}
	z_{1} \\
	z_{2} \\ 
	z_{3} \\
	z_{4} \\
	\cdot 
      \end{bmatrix} \le
      \begin{bmatrix}
	M \\
	1 \\ 
	1 \\
	1 \\
	\cdot 
      \end{bmatrix}
    \end{split}
  \end{equation*}
  So we would like to have an algortihm that gurntess that the current solution (fraction LP version) for one of the program is valid but sull serves as a good approximation to the other. Consider the following algorithm ( $c$ in the end will be equals to $2$ ): 

  %\fbox {
  \begin{algorithm}[H]
    \caption{Ski-Rental}
    \label{alg:three}
    %\KwData{ $x \in \mathbb{F}_{2}^{n}$ }
    %\KwResult{ $\arg\min {\left\{  y \in C : |y + x|  \right\} }$ if $d(y,C) < \tau $ and False otherwise. }
    %$ L \leftarrow \text{Array} \{ \} $\\
    \For { each new day $j$  } {
      \If{ $x < 1$  } { 
	$ \xi_{j} \Leftarrow 1 - x$ \\
	$ x \Leftarrow \left( 1 + \frac{1}{M} \right) x + \frac{1}{\left( c - 1 \right) M } $ \\
	$ z_{j} \Leftarrow 1$ \\ 
	$\tilde{z_{j}} \leftarrow z_{j} / \left(  1 + \frac{c}{M} \right)  $ 
      }
  }
  \end{algorithm}
  
  \paragraph{}

By the fact that the algorithm halt only after $x \ge 1 $ it follows that $x$ is a valid solution for the fraction primal program, and therefore is also an upper bound for dual program. 

Also Notice that sacling $\tilde{z}_{i} \leftarrow z_{i} \cdot M / \left( \sum_{i}z_{i}  \right)$, is a vaild soultion for primal program. Denote by $k$ the last day in which  $x < 1$. Namly $z_{j} = 0 $ for any $j > k$, Then we have that :  $x < \left( 1 + 1/M  \right)\cdot 1 + 1 / \left( \left( c-1 \right)M \right) $ So picking $c = 2 \Rightarrow x < 1 + \frac{2}{M}$.
  Now As in each itearion we add at most : 
  \begin{equation*}
    \begin{split}
      \Delta  & = x_{t+1} - x_{t} \sim \frac{1}{M} x_{t} + \frac{1}{M} \Rightarrow \frac{k}{M} < 1+ \frac{2}{M} \\
      \Rightarrow & k \le M\left( 1 + \frac{2}{M} \right) \Rightarrow \sum{z_{j}}\le M\left( 1 + \frac{2}{M} \right)
    \end{split}
  \end{equation*}
So $\tilde{z_{j}}$ are valid soultion to the dual fraction program such is $ \left( 1 + \frac{2}{M} \right)$ approximate. As the scaling made by a constant factor, one could ensure to hold the current approximate soultion in each iteration.   
So it left to show how one can transfrom the fractional version into randomized algorithm that need to choose etiher rent or buy. We will do that by flip a coin in each iteartion at probability $x$. I do not know how to compute the expected cost, 
%}
  \paragraph{ex5.} Prove Yao's minimax principle. 

  \begin{equation*}
    \begin{split}
      & \forall \text{rand. } \tilde{\text{alg}} \  \exists  \ \sigma  \\
      & \ \ \dalg{ \calg\left( \sigma \right) } \ge c \cdot   \cbase\left( \sigma \right) \\  
      & \Leftrightarrow \exists \ \text{rand.} \ \tilde{\sigma} \forall \text{alg} \\  
      & \ \ \dsig{  \calg\left( \sigma \right)  } \ge c \dsig{ \cbase \left( \sigma \right)  } 
    \end{split}
  \end{equation*}
  \paragraph{Solution.} First direction, assume through contradiction that there exists a deterministic algorithm such that for all distributions $\tilde{\sigma}$ :
  \begin{equation*}
    \begin{split}
      \dsig{  \calg\left( \sigma \right)  } < c \dsig{ \cbase \left( \sigma \right)  }
    \end{split}
  \end{equation*} And that holds, in paritcular, for distribution $\tilde{\sigma}$ which suportted by a single $\sigma$. Hence, because any deterministic algorithm is also a randomized algorithm, set it to be $ \tilde{\text{alg}}$, and that immediately yields a contradiction. 
  It is left to show the second direction. By the monotonic property of random variables, we have that for any distribution $\tilde{\sigma}$: 
  \begin{equation*}
    \begin{split}
      &  \dalg{ \dsig{ \calg\left( \sigma \right) } } \\
      & \ \  \ \ \ge c \cdot \dalg{ \dsig{  \cbase\left( \sigma \right) } } \\
      & \dsig{ \dalg{ \calg\left( \sigma \right) } } \\
      &  \ \  \ \ \ge c \cdot \dsig{ \dalg{  \cbase\left( \sigma \right) } } \\
      & \dsig{ \dalg{ \calg\left( \sigma \right) } } \\
      & \ \ \ \  \ge c \cdot \dsig{  \cbase\left( \sigma \right) }  
    \end{split}
  \end{equation*}
  And by the fact that inequality of exception between random variables follows an existence of atomic event on which the inequality holds, we obtain that there must exists at least a single $\sigma$ such that:
  \begin{equation*}
    \begin{split}
      \dsig{ \dalg{ \calg\left( \sigma \right) } } \ge c \cdot \dsig{  \cbase\left( \sigma \right) }  
    \end{split}
  \end{equation*}
  And that ends the proof.
\end{multicols*}
\printbibliography 
\end{document}








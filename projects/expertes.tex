\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={7.5in, 10in} ]{geometry}
\usepackage{braket}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{patterns, shapes.arrows}
\usepackage{adjustbox}
\usepackage{tikz-network}
\usepackage[linesnumbered]{algorithm2e}
\usepackage{multicol}
\usepackage[backend=biber,style=alphabetic,sorting=ynt]{biblatex}
\usepackage{xcolor}
\usepackage{pgfplots}
\DeclareUnicodeCharacter{2212}{âˆ’}
\usepgfplotslibrary{groupplots,dateplot}
\pgfplotsset{compat=newest}

\addbibresource{sample.bib} %Import the bibliography file

\newcommand{\commentt}[1]{\textcolor{blue}{ \textbf{[COMMENT]} #1}}
\newcommand{\ctt}[1]{\commentt{#1}}
\newcommand{\prb}[1]{ \mathbf{Pr} \left[ {#1} \right]}
\newcommand{\expp}[1]{ \mathbf{E} \left[ {#1} \right]}
\newcommand{\onotation}[1]{\(\mathcal{O} \left( {#1}  \right) \)}
\newcommand{\ona}[1]{\onotation{#1}}
\newcommand{\PSI}{{\ket{\psi}}}
\newcommand{\LESn}{\ket{\psi_n}}
\newcommand{\LESa}{\ket{\phi_n}}
\newcommand{\LESs}{\frac{1}{\sqrt{n}}\sum_{i}{\ket{\left(0^{i}10^{n-i}\right)^{n}}}}
\newcommand{\Hn}{\mathcal{H}_{n}}
\newcommand{\Ep}{\frac{1}{\sqrt{2^n}}\sum^{2^n}_{x}{ \ket{xx}}}
\newcommand{\HON}{\ket{\psi_{\text{honest}}}}
\newcommand{\Lemma}{\paragraph{Lemma.}}
\newcommand{\PonB}{ \rho + \frac{5}{16}\delta\le \frac{3}{4} + \frac{1}{16} } 
\newcommand{\Cpa}{[n, \rho n, \delta n]}
%\setlength{\columnsep}{0.6cm}
\newcommand{\Jvv}{ \bar{J_{v}} } 
\newcommand{\Cvv}{ \tilde{C_{v}} } 

\newcommand{\Gz}{ G_{z}^{\delta} } 
\newcommand{ \Tann } {  \mathcal{T}\left( G, C_0 \right) }
\begin{document}



\newcommand{\dalg}[1]{\expp{#1 : \text{alg} \sim \tilde{\text{alg}}}}
\newcommand{\dsig}[1]{\expp{#1 : \sigma \sim \tilde{\sigma}}}
\newcommand{\calg}{c_{\text{alg}}}
\newcommand{\cbase}{c_{\text{base}}}


\title{Online Computation, Armed Bandit.} 
\author{David Ponarovsky}
\maketitle
%\abstract{We propose a new simple construction based on Tanner Codes, which yields a good LDPC testable code.} 

\begin{multicols*}{2}
  \paragraph{multi armed bandits}
  \begin{enumerate}
    \item $n$ arms.
    \item $l_{i,t}$ and $g_{i,t}$ are the lost and the gain function of the $i$th arm at time $t$.    \item In each time step $t$ we learn only the $t$th coordinate of the arm we chooses.     
  \end{enumerate}
  When we choose $i_{t}$ we only learn $l_{i},t$, unlikely of the experts case when we learn the loss of all the experts, here we only learn the loss of the arm that we choose, we don't know what is the loss of the others arms. So it seems like hopeless situation, and deterministic that indeed the case, but it turns out that using randomness we can actually do something. 

  So, we will aim at competing against adaptive adversary, But unfortunately here it is more complicated situation, so we will discuss at first an oblivious adversary. So what is mean that our adversary is oblivious in that case? The adversary could choose the loss vector depends on the our last chooses in past, but he doesn't know what is going be our current choose. So if our choices are random variables than the loss function is also going to be a random variable.    

  We measure the performance by a regret function define by:  

  \begin{equation*}
    \begin{split}
      R_{T} = \sum_{t=1}^{T}{l_{i_{t},t}} - \min \sum_{t=1}^{T}{l_{j,t}}
    \end{split}
  \end{equation*}
  When $T$ count the number of steps. The goal is to bound the $\expp{R_{T}}$. Notice that algorithm hasn't the information to calculate that quantity. In general good algorithms combine two consideration. So first at all they want to exploit arms that have low cost, But if we just exploit arms that seems to be promise in the begging we might ends with choice that has an heigh cost. So there is must to be an element of exploration, and this is the secret. Blanching right between exploration and exploition. 

  Another representation of the Regert is the difference between the score we gained and the gained which achieved by gamble on the best bandit.  
\begin{equation*}
    \begin{split}
      R_{T} = \max \sum_{t=1}^{T}{g_{j,t}}-\sum_{t=1}^{T}{g_{i_{t},t}} 
    \end{split}
  \end{equation*}
 Let's define the pseudo-regret, 

 \begin{equation*}
   \begin{split}
     \bar{R}_{T} = \expp{ \sum_{t=1}^{T}{l_{i_{t},t}}} - \expp{ \min \sum_{t=1}^{T}{l_{j,t}}} \le \expp{R_{T}}  
   \end{split}
 \end{equation*}
 So what we are planning to do, is first to design an algorithm that has a good pseudo-regret gurntes. But that doesn't says anything about the regret gurntess. But that will be our starting point, and when the adversary is a oblivious the pseudo regret and the regret are equivalence. So one way to think about that is that we looking for way to bound the regret against the oblivious adversary and we think how we can improve the algorithm against a non oblivious adversary. 

 We will show that, at least amortized, the regret per step will be a little $o\left( 1 \right)$.  In the experts case the regret after $T$ steps were something like $\sqrt{T\log\left( n \right)}$. Here the factor is going to be something like $O\left( n\log n \right)$. This is not surprising, because in the experts case we learn all the value, while here we learn only a single bit of information, So it make sense that we will have to pay a linear factor of time till the converge into the stage in which the regret is arbitrary small. 

 The first algorithm is called \textbf{EXP3}.

 So how that algorithm is operating? First we initialize for any $i \in \{ 1 , .. , n\}$, $\tilde{L}_{i,0} \leftarrow 0 $. We are going to mimic the multiplicative update or Hedge in the experts case, the problem is that we don't know the values. Because, the update in the experts case require to know the loss of all the experts. To overcome over that, we will try to estimate the loss of the other by unbias estimators, or learn the $L$'s function by the way.   
 In addition, we initialize a distribution over the choosing the arms $P_{t}$ which in time zero is just the uniform distribution over the arms. 

 \begin{algorithm}[H]
   for $i \in [n]$ set  $\tilde{L}_{i,0} \leftarrow 0 $\\
   set $P_{t}$ to the uniform distribution on $\{1,2 .. n \}$ \\ 
   \For{ $t \in 1,2 .., T$ }{
     draw $t_{t}$ from $P_{t}$ \\
     \For{$i \in [n]$ }{
       $\tilde{L}_{i,t} \leftarrow \tilde{L}_{i,t-1} + \mathbf{1}_{i_{t} = i} \cdot \frac{l_{i,t}}{P_{i,t}}$ \\ 
     }
     update the probability $P_{i,t+1} \leftarrow \frac{e^{-\eta_{t}\tilde{L}_{i,t} }}{ \sum_{j}{e^{-\eta_{t}\tilde{L}_{i,t} }} }$ \\ 
     the $\eta_{t}$ are non-increasing sequence \\ 
     \ of values in $(0,\infty)$ that will determined later. 
   }
 \end{algorithm}
 The $\eta{t}$ is called the learning rate. We will see two different versions, the first with fixed learning rate and the other about choosing a learning rate that goes down. 

 \paragraph{Claim.} $\tilde{l}_{i,t}$ is unbiased estimator of$l_{i,t}$. 
 \paragraph{Proof.}
 Let's proof that, we just have to show that the expectation of that value over the possibilities is just $l_{i,t}$. $\expp{\tilde{l}_{i,t}} = \frac{l_{i,t}}{P_{i,t}} \cdot \prb{i_{t}=i} = l_{i,t}$. When in the last pass we use the fact that the algorithm draw from $P_{t}$.  

 \paragraph{Claim.} For every non-increasing $\{\eta\}$, \mathbf{EXP3} satisfies the following ineqaulity:  
 \begin{equation*}
   \begin{split}
     \bar{R}_{T} \le \frac{n}{2}\sum_{t=1}^{T}{\eta_{t}} + \frac{\ln(n)}{\eta_{T}} 
   \end{split}
 \end{equation*}
 \paragraph{Corollary.} We can set the following two bounds.\begin{enumerate}
   \item For $\eta_{t} = \sqrt{\frac{\ln(n)}{ nT} }$ we have that $\bar{R}_{T} \le \sqrt{2Tn\ln(n)}$.  
   \item  For $\eta_{t} = \sqrt{\frac{\ln(n)}{ nt} }$ we have that  $\bar{R}_{T} \le 2 \sqrt{Tn\ln(n)}$ (expand by factor of $\sqrt{2}$).
 \end{enumerate} 
 The disadvantage of the first bound, is that using it require us knowing what is the number of the steps for initialized the rate. This result tell us that if we compete against oblivious adversary after $n\log(n)$ steps the regret increase by $o(1)$. 

 \paragraph{Proof.} Recall that $\tilde{l}_{i,t}$ is an unbiased estimator of $l$ and notice that $\expp{\left( \tilde{l}_{i,t} \right)^2} = \frac{l_{i,t}^{2}}{P_{i,t}}$ and that $\expp{\frac{1}{p_{i,t}}} = n$. Lets look on the sum $\sum^{T}_{t=1}{l_{i_{t},t}} - \sum_{t=1}^{T}{l_{j,t}}$

 \begin{equation*}
   \begin{split}
 & \sum^{T}_{t=1}{l_{i_{t},t}} - \sum_{t=1}^{T}{l_{j,t}} =\sum^{T}_{t=1}{\expp{ \tilde{l}_{i_{t},t}}} - \sum_{t=1}^{T}{\expp{\tilde{l}_{j,t} }} \\
 & \expp{ \tilde{l}_{i_{t},t}} = \frac{1}{\eta_{t}}\ln\left( e^{\eta_{t}\expp{\tilde{l}_{i,t}}} \right) + \frac{1}{\eta_{t}}\ln\expp{e^{-\eta_{t}\tilde{l}_{i,t}}}-\frac{1}{\eta_{t}}\ln\expp{e^{-\eta_{t}\tilde{l}_{i,t}}} \\ 
& =\frac{1}{\eta_{t}}\ln\expp{e^{\eta_{t}\expp{\tilde{l}_{i,t}}-\eta_{t}\tilde{l}_{i,t}}} -\frac{1}{\eta_{t}}\ln\expp{e^{-\eta_{t}\tilde{l}_{i,t}}} \\ 
& =  
 \end{split}
 \end{equation*}
\end{multicols*}
  \printbibliography 
\end{document}







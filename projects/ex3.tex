\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={7.5in, 10in} ]{geometry}
\usepackage{braket}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{patterns, shapes.arrows}
\usepackage{adjustbox}
\usepackage{tikz-network}
\usepackage[linesnumbered]{algorithm2e}
\usepackage{multicol}
\usepackage[backend=biber,style=alphabetic,sorting=ynt]{biblatex}
\usepackage{xcolor}
\usepackage{pgfplots}
\DeclareUnicodeCharacter{2212}{âˆ’}
\usepgfplotslibrary{groupplots,dateplot}
\pgfplotsset{compat=newest}

\addbibresource{sample.bib} %Import the bibliography file

\newcommand{\commentt}[1]{\textcolor{blue}{ \textbf{[COMMENT]} #1}}
\newcommand{\ctt}[1]{\commentt{#1}}
\newcommand{\prb}[1]{ \mathbf{Pr} \left[ {#1} \right]}
\newcommand{\expp}[1]{ \mathbf{E} \left[ {#1} \right]}
\newcommand{\onotation}[1]{\(\mathcal{O} \left( {#1}  \right) \)}
\newcommand{\ona}[1]{\onotation{#1}}
\newcommand{\PSI}{{\ket{\psi}}}
\newcommand{\LESn}{\ket{\psi_n}}
\newcommand{\LESa}{\ket{\phi_n}}
\newcommand{\LESs}{\frac{1}{\sqrt{n}}\sum_{i}{\ket{\left(0^{i}10^{n-i}\right)^{n}}}}
\newcommand{\Hn}{\mathcal{H}_{n}}
\newcommand{\Ep}{\frac{1}{\sqrt{2^n}}\sum^{2^n}_{x}{ \ket{xx}}}
\newcommand{\HON}{\ket{\psi_{\text{honest}}}}
\newcommand{\Lemma}{\paragraph{Lemma.}}
\newcommand{\PonB}{ \rho + \frac{5}{16}\delta\le \frac{3}{4} + \frac{1}{16} } 
\newcommand{\Cpa}{[n, \rho n, \delta n]}
%\setlength{\columnsep}{0.6cm}
\newcommand{\Jvv}{ \bar{J_{v}} } 
\newcommand{\Cvv}{ \tilde{C_{v}} } 

\newcommand{\Gz}{ G_{z}^{\delta} } 
\newcommand{ \Tann } {  \mathcal{T}\left( G, C_0 \right) }
\begin{document}



\newcommand{\dalg}[1]{\expp{#1 : \text{alg} \sim \tilde{\text{alg}}}}
\newcommand{\dsig}[1]{\expp{#1 : \sigma \sim \tilde{\sigma}}}
\newcommand{\calg}{c_{\text{alg}}}
\newcommand{\cbase}{c_{\text{base}}}


\title{Online Computation, Ex 3.} 
\author{David Ponarovsky}
\maketitle
%\abstract{We propose a new simple construction based on Tanner Codes, which yields a good LDPC testable code.} 

\begin{multicols*}{2}
  \paragraph{ex1.} Consider the experts setting with gains: $g_{i,t} \in \left[ 0,1 \right]$ is the gain of expert $i$ at step $t$. Hedge updates:  
  \begin{equation*}
    \begin{split}
      P_{i,t+1}= \frac{e^{\eta G_{i,t}}}{\sum_{j}{ e^{\eta G_{j,t}}} }
    \end{split}
  \end{equation*} where $G_{i,t} = \sum_{s\le t}{g_{i,t}} $. Prove that the regret of Hedge at time $T$ is $O\left( \sqrt{T\log n} \right)$, for a good choice of the learning rate $\eta$, against the adaptive adversary.  
  %  different time steps $t,t^{\prime}$ the random varibels $g_{t},g_{t^{\prime}}$ are independent. That fact follows an immediately drawn process. Namely, in each turn, the probabilities are recomputed according to the known values, So choice at time $t$ couldn't impact the distribution at time $t^{\prime}$.
  \paragraph{Solution.}
  Let $g_{t}$ be the random variable which counts the gain at time step $t$ and by $G_{t} = \sum_{t}^{T}{g_{t}}$. Recall that for any pair of random variables $X, Y$ such that $X \ge Y$ holds that $\expp{X} \ge \expp{Y}$. Also notice that for $x$ restricted to some range $[-r,r]$ there are constants $c_{0}, c_{1}$ depend on $r$ such that $ e^{x} - 1 - x \le c_{0}x^{2}$ and $1 + x + c_{0}x^{2} \le e^{x+c_{1}x^{2}}$. Namely, the exponent is bounded by quadric approximation (second Tylor series order). By the monotonous property of the expectation, for any random variable $X$ that maps to bounded range $[-r,r]$, it holds that:
  \begin{equation*}
    \begin{split}
      \expp{e^{x}} \le \expp{ 1 + x + c_{0}x^{2}} \le  e^{ \expp{ x+c_{1}x^{2}}}
    \end{split}
  \end{equation*}
  Define the potential $\psi\left( t \right) =  \sum_{j}{ e^{\eta G_{i,t}}}$ and notice that:
    \begin{enumerate}
      \item $ \frac{\psi\left( t+1 \right)}{\psi\left( t \right)} = \expp{e^{\eta g_{t}}}$ relatives to the distribution $P_{i,t+1}$. 
      \item $ \psi\left( t \right) \ge e^{\eta G_{t,j}} $ for any $t$ and $j$ in particular the $j$ which maximizes the gain. 
%      \item $ \frac{\psi\left( t+1 \right)}{\psi\left( t \right)} \le e^{\eta}$
%      \item $ e^{\eta G_{t,j}} \le e^{\eta} \psi\left( 0 \right)$ for any $j$.
    \end{enumerate}
Therefore we obtain that: 
\begin{equation*}
  \begin{split}
    & \psi\left( T \right) =  \frac{\psi\left( T \right)}{\psi\left( 0 \right)}\psi\left( 0 \right) = \prod^{T}{\frac{\psi\left( t+1 \right)}{\psi\left( t \right)} }\psi\left( 0 \right) \\
  & n \prod^{T}_{t}{\expp{ e^{\eta g_{t}} }}\le n \prod^{T}_{t}{\expp{ 1 + \eta g_{t} + c_{\pm}\left( \eta g_{t} \right)^{2} }} \\
& n \prod^{T}_{t}{ 1 + \expp{ \eta g_{t} + c_{\pm}\left( \eta g_{t} \right)^{2} }} \le  
 n \prod^{T}_{t}{ e^{\expp{ \eta g_{t} + c_{\pm}\left( \eta g_{t} \right)^{2} }}} \le \\ 
& n e^{\expp{ \sum {\eta g_{t} + c_{\pm}\left( \eta g_{t} \right)^{2} }}} \le n e^{\expp{ \sum {\eta g_{t}}} +\expp{{ c_{\pm}\left( \eta g_{t} \right)^{2} }}}
  \end{split}
\end{equation*}
On the other hand, by the second property, it follows that for any $j$: 
\begin{equation*}
  e^{\eta G_{j,T}} \le n e^{\expp{ \sum {\eta g_{t}}} +\expp{{ c_{\pm}\left( \eta g_{t} \right)^{2} }}}
\end{equation*} 
By dividing at $e^{\expp{ \sum {\eta g_{t}}}}$, extracting the logarithm and combine the fact that $g_{t}^2 = g_{t}$ (indicator) we have that: 
\begin{equation*}
  \begin{split}
    R_{T} \le \frac{1}{\eta}\log\left( n \right) + c_{+}\eta T
  \end{split}
\end{equation*}
And by choosing $\eta = \sqrt{\log\left( n \right)/T }$ we complete the proof. 
%  by the fact that $e^{x}$ is positive function we have that $\psi\left( t \right) \ge e^{\eta \max_j G_{i,j}}$. In addition: 
%  \begin{equation*}
%    \begin{split}
%      & \frac{\psi\left( t+1 \right)}{\psi\left( t \right)} = \psi\left( t \right)^{-1} \sum_{j}{ e^{\eta G_{j,t} + \eta g_{j,t+1}}} \\
%      & = \expp{e^{\eta g_{t}}} \le e^{\expp{\eta g_{t}}} \Rightarrow \psi\left( T \right) \le \psi\left( 0 \right) e^{\eta \expp{G_{t}}} \\
%      & \psi\left( t + 1 \right) =  \sum_{j}{ e^{\eta G_{j,t+1}}} =\sum_{j}{ e^{\eta G_{j,t} + \eta g_{j,t+1}}}  \\
%      %& \Rightarrow   \psi\left( t \right) \le \psi\left( t + 1 \right)\le \psi\left( t \right)e^{\eta} \\ 
%      & \le \sum_{j}{ e^{\eta G_{j,t}  }\left(  1 + c_{1} \eta g_{j,t+1} \right) }= \psi\left( t \right)  +  c_{1}\eta \psi\left( t \right) \expp{g_{t+1}}  \\
%      & \le  e^{\eta} \left( 1 +  c_{2} \right) \psi\left( t \right) \expp{g_{t+1}}  \\
%      & \le \prod_{t}{ \left( 1 + c_{1} \expp{g_{t}}\right) }\left( \psi\left( 0 \right) \right) \le e ^{c_{2} \expp{ \sum{g_{t} } }}\psi\left(0  \right) \\ 
%      & \le e^{c_{2}\expp{G_{t}}} \cdot e^{\eta}n 
%    \end{split}
%  \end{equation*}
%  So after $T$ steps, by taking the logarithm of both sides, we obtain that the regret is bounded by $ R_{T} \le   $.
  \paragraph{ex2.} Show a lower bound of $\Omega\left( \sqrt{T} \right)$ in the experts setting on the regret of any online algorithm against the oblivious adversary. 

  \paragraph{Solution.} Consider an adversarial which draw the values of $g_{i,t}$ uniformly random, in particular $g_{i,t}$'s are independent. Fix an online algorithm for the problem and denote by $g_{t}$ the gain that earns by it at time step $t$. As $g_{i,t}$ are independent, the sum $G_{T} = \sum{g_{t}}$ is a summation of independent variables with the same exception and variance. Therefore we know that $\left( G_{T} - T\mu \right)/ \sqrt{T} \sim G\left( 0 , \sigma \right)$ where $\mu$ and $\sigma$ do not depends on $T$. Denote that Gaussian by $X$.

On the other hand, run in which the optimal gain $T\mu+\frac{1}{2}\sqrt{T}$ might occur with positive probability. Using that event, we infer that the regret has to be at least: 
  \begin{equation*}
    \begin{split}
      R_{T} \ge T  \mu  + \frac{1}{2}\sqrt{T}- \expp{G_{t}} = \frac{1}{2}\sqrt{T}
    \end{split}
  \end{equation*}
  \paragraph{ex3.}Consider a system of linear inequalities $Ax \ge b$, where $A\in [0, \infty]^{m\times n} , b \in  [0, \infty]^{m}$, and unknown $x \in  [0, \infty]^{ n}$. (we are seeking a non-negative solution). An $\varepsilon$-approximate solution $x\ge 0$ satisfies $Ax\ge b - \varepsilon \mathbf{1}$. Suppose we have an efficient procedure for the following problem: Given $p \in [0,1]^{m}, \sum_{i \in [m]}{p_{i}} = 1$, decide if exists $x \ge 0, p^{\top}Ax \ge p^{\top}b$. Show how to find an $\varepsilon$-approximate solution to $Ax \ge b$. Analyze the run-time. 
  \paragraph{Solution.} I think that has been misunderstood, and the restriction of $A > 0 $ was written by mistake. Because otherwise, One could pick $x$ to be the vector $x_{i} = \frac{\max{b}}{\min{A}}$ for any $i$, where the minimum is taken over all the non-zero values of $A$. Notice that on the one hand, $x \ge 0 $ and on the other hand, the inequality is satisfied. For see that, consider non-zero row $a_{i}$ of $A$, there is must to be at least one entry $A_{ij^{\prime} \in a_{i}}$ which is nonzero, Hence we have that: 
  \begin{equation*}
    \begin{split}
      \left( Ax \right)_{i} = \sum_{A_{ij} \frac{\max b }{ \min A} } \ge \frac{A_{ij^{\prime}}}{\min{A}}\max {b} \ge b_{i}
    \end{split}
  \end{equation*}
  Also, the above proves that the inequalities system has no solution only if a coordinate exists for which $b_{i} > 0$ and the $i$th row contains only zero values.  
  One can Compute $x$ by doing at most one iteration over the input, So the total running time is at most $O\left( m n \right)$. My guess is that if $A$ isn't restricted to positive values, then the problem is equivalence to solving an arbitrary LP. 
  \paragraph{ex4.} Recall that we showed, for $EXP$ updates, that w.p $1 - \delta$ 
  \begin{equation*}
    \begin{split}
      &  R{T} \le \beta n T + \gamma T + \left( 1 + \beta \right)\eta + \frac{\ln\left( \delta^{-1} n \right) }{\beta} + \frac{\ln n}{\eta}  
    \end{split}
  \end{equation*}
  Infer that for the right choice of $\beta,\gamma, \eta$ 
  \begin{equation*}
    \begin{split}
      \expp{R_{T}} = O\left( \sqrt{T n \ln n } \right)
    \end{split}
  \end{equation*}
  \paragraph{Solution.} Let's choose $\delta = 2^{-Tn}$, $\beta = \sqrt{\frac{log n }{nT}}$, and $\gamma, \eta = \Theta\left( \beta \right)$ assume that $T = \Omega(n)$ (which is reasonable assumption). Observes that the term $\frac{\log\left( \delta^{-1}n \right)}{\beta}$ becomes $ \frac{log\left( n \right)}{\beta} + \frac{1}{nT\beta} $ and then we obtain that:  
  \begin{equation*}
    \begin{split}
      & \expp{R_{T}} \le \left( 1 - 2^{-Tn} \right)\Theta\left( \sqrt{ Tn\log n} \right)+ 2^{-Tn} \cdot T = \\ 
      & \Theta\left( \sqrt{ Tn\log n} \right)
    \end{split}
  \end{equation*}
\end{multicols*}
  \printbibliography 
\end{document}







